{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Using Count Vectors and TFIDF-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train,df_test = train_test_split(df,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer can lowercase letters, disregard punctuation and stopwords, but it can't LEMMATIZE or STEM ...\n",
    "\n",
    "Analysing Count Vectorizer ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature in Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", \n",
    "                            analyzer='word', \n",
    "                            ngram_range=(1, 1), \n",
    "                            max_df=1.0, \n",
    "                            min_df=1, \n",
    "                            max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [df_train[\"comment_text\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature:\n",
      "['205', '27', '38', '89', 'closure', 'dolls', 'don', 'edits', 'explanation', 'fac', 'fan', 'gas', 'hardcore', 'just', 'metallica', 'new', 'page', 'remove', 'retired', 'reverted', 'talk', 'template', 'username', 'vandalisms', 'voted', 'weren', 'york']\n"
     ]
    }
   ],
   "source": [
    "count_train = count_vec.fit(text)\n",
    "bag_of_words = count_vec.transform(text)\n",
    "\n",
    "print(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary and Vocabulary ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 27 \n",
      "Vocabulary content:\n",
      " dict_keys(['explanation', 'edits', 'username', 'hardcore', 'metallica', 'fan', 'reverted', 'weren', 'vandalisms', 'just', 'closure', 'gas', 'voted', 'new', 'york', 'dolls', 'fac', 'don', 'remove', 'template', 'talk', 'page', 'retired', '89', '205', '38', '27'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size : {} \".format(len(count_train.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. N-Grams (sets of consecutive words) [N=(1,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\",ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [df_train[\"comment_text\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vec.fit(text)\n",
    "bag_of_words = count_vec.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['205', '205 38', '27', '38', '38 27', '89', '89 205', 'closure', 'closure gas', 'dolls', 'dolls fac', 'don', 'don remove', 'edits', 'edits username', 'explanation', 'explanation edits', 'fac', 'fac don', 'fan', 'fan reverted', 'gas', 'gas voted', 'hardcore', 'hardcore metallica', 'just', 'just closure', 'metallica', 'metallica fan', 'new', 'new york', 'page', 'page retired', 'remove', 'remove template', 'retired', 'retired 89', 'reverted', 'reverted weren', 'talk', 'talk page', 'template', 'template talk', 'username', 'username hardcore', 'vandalisms', 'vandalisms just', 'voted', 'voted new', 'weren', 'weren vandalisms', 'york', 'york dolls']\n"
     ]
    }
   ],
   "source": [
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. N-Grams (sets of consecutive words) [N=(1,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\" , ngram_range = (1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vec.fit(text)\n",
    "bag_of_words = count_vec.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['205', '205 38', '205 38 27', '27', '38', '38 27', '89', '89 205', '89 205 38', 'closure', 'closure gas', 'closure gas voted', 'dolls', 'dolls fac', 'dolls fac don', 'don', 'don remove', 'don remove template', 'edits', 'edits username', 'edits username hardcore', 'explanation', 'explanation edits', 'explanation edits username', 'fac', 'fac don', 'fac don remove', 'fan', 'fan reverted', 'fan reverted weren', 'gas', 'gas voted', 'gas voted new', 'hardcore', 'hardcore metallica', 'hardcore metallica fan', 'just', 'just closure', 'just closure gas', 'metallica', 'metallica fan', 'metallica fan reverted', 'new', 'new york', 'new york dolls', 'page', 'page retired', 'page retired 89', 'remove', 'remove template', 'remove template talk', 'retired', 'retired 89', 'retired 89 205', 'reverted', 'reverted weren', 'reverted weren vandalisms', 'talk', 'talk page', 'talk page retired', 'template', 'template talk', 'template talk page', 'username', 'username hardcore', 'username hardcore metallica', 'vandalisms', 'vandalisms just', 'vandalisms just closure', 'voted', 'voted new', 'voted new york', 'weren', 'weren vandalisms', 'weren vandalisms just', 'york', 'york dolls', 'york dolls fac']\n"
     ]
    }
   ],
   "source": [
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. TfIDFVectorizer \n",
    "\n",
    "formula used:\n",
    "             tf-idf(d, t) = tf(t) * idf(d, t)\n",
    "\n",
    "            * tf(t)= the term frequency is the number of times the term appears in the document\n",
    "            * idf(d, t) = the document frequency is the number of documents 'd' that contain term 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text:  ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "txt_fitted = tf.fit(txt1)\n",
    "txt_transformed = txt_fitted.transform(txt1)\n",
    "print (\"The text: \", txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his': 1.4054651081081644, 'not': 1.0, 'perfect': 1.4054651081081644, 'sang': 2.09861228866811, 'she': 2.09861228866811, 'smile': 1.4054651081081644, 'was': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "tf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = tf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his': 1.4054651081081644, 'not': 1.0, 'perfect': 1.4054651081081644, 'sang': 2.09861228866811, 'she': 2.09861228866811, 'smile': 1.4054651081081644, 'was': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "print(dict(zip(txt_fitted.get_feature_names(), idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf: \n",
      "['not' 'his' 'perfect']\n",
      "Features with highest idf: \n",
      "['was' 'sang' 'she']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(tf.get_feature_names())\n",
    "sorted_by_idf = np.argsort(tf.idf_)\n",
    "\n",
    "print(\"Features with lowest idf: \\n{}\".format(feature_names[sorted_by_idf[:3]]))\n",
    "print(\"Features with highest idf: \\n{}\".format(feature_names[sorted_by_idf[-3:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
